#Detecting Exoplanets amongst Kepler Objects of Interest

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(GGally)
library(bestglm)
library("pROC")
library(kableExtra)
library(crayon)
library(FNN)
```

#Introduction

Extrosolar planets may be discovered by two commonly-used methods: the first (the transit method) involves detecting the exoplanet as it partially eclipses host star, and the second (the radial velocity method) involves detecting the host star's "wobble" around its planetary system's center of mass. Potential such host stars are referred to as "Kepler objects of interest". The data given to us contains information on a set of 9177 Kepler objects of interest, with 18 predictor variables that measure aspects related to exoplanets, host stars, exoplanet orbits, and transit, and a response variable that states for each observation if we have detected a confirmed exoplanet ("CONFIRMED"), have not identified an exoplanet ("FALSE POSITIVE"), or are unsure of the nature of what we have detected ("CANDIDATE"). The descriptions of the predictors are given below:

```{r echo = FALSE}
names.vec = c("koi_period", "koi_eccen", "koi_impact", "koi_duration", "koi_depth", "koi_ror", "koi_srho", "koi_prad", "koi_sma", "koi_incl", "koi_teq", "koi_insol", "koi_dor", "koi_steff", "koi_slogg", "koi_smet", "koi_srad", "koi_smass")
descrip.vec = c("The interval between consecutive planetary transits", "Eccentricity value of the exoplanet (i.e. the ratio of the distance from the center to the foci and the distance from the center to the vertices)", "The sky-projected distance between the center of the stellar disc and the center of the planet disc at conjunction, normalized by the stellar radius", "The duration of the observed transits", "The fraction of stellar flux lost at the minimum of the planetary transit", "The planet radius divided by the stellar radius", "The fitted stellar density", "The radius of the planet", "Half of the long axis of the ellipse defining a planet's orbit", "The angle between the plane of the sky (perpendicular to the line of sight) and the orbital plane of the planet candidate", "Approximation for the temperature of the planet", "The insolation flux", "The distance between the planet and the star at mid-transit divided by the stellar radius", "The photospheric temperature of the star", "The base-10 logarithm of the acceleration due to gravity at the surface of the star", "The base-10 logarithm of the Fe to H ratio at the surface of the star, normalized by the solar Fe to H ratio", "The photospheric radius of the star", "The mass of the star")
kable(cbind("Names" = names.vec, "Descriptions" = descrip.vec), format = "html", caption = "Predictors in our Dataset") %>% kable_styling()
```

Our aim is to identify a model that may be trained in order to effectively differentiate between "CONFIRMED" and "FALSE POSITIVE" exoplanetary candidates. We will attempt 6 models - logistic regression, linear discriminant analysis, classification tree, random forest, Naive bayes, and XGBoost - and will select the one with the best performance to apply to our candidate data.

```{r message = FALSE, echo = FALSE, include = FALSE, warning = FALSE}
rm(list=ls())
file.path = "/Users/ananya/Desktop/OneDrive/Ananya/FALL 2019/36-290/Git/KeplersObjects/KEPLER_OBJS_INTEREST/koi.Rdata"
load(file.path)
df = data.frame(predictors,"y"=response)
rm(file.path)
objects()
```


#Summary of given data

Firstly, looking at our dataset, it appears that koi_eccen has only zeroes in it, so we may remove this predictor as it will likely not contribute much to our analysis. Next, we must remove the rows which have a response of "CANDIDATE" from our dataset as we are only interested in predicting our responses as either "CONFIRMED" or "FALSE POSITIVE". We will also split our data into test and training sets, keeping 80% of the data in our training set and the other portion in our test set.

```{r echo = FALSE}
set.seed(101)
df$koi_eccen = NULL
predictors$koi_eccen = NULL
response = droplevels(response, "CANDIDATE")
indices.resp = which(response== "CONFIRMED" | response== "FALSE POSITIVE")
response = response[indices.resp]
rm.indices = which(df$y == "CANDIDATE")
df.candidate = df[rm.indices,]
pred.main = predictors
resp.main = response
df.main = df
df = df[-rm.indices,]
predictors = predictors[-rm.indices,]
indices = sample(c(1:nrow(predictors)),0.8*nrow(predictors))
pred.train = predictors[indices,]
resp.train = response[indices]
resp.test = response[-indices]
pred.test = predictors[-indices,]
```


Now, we will conduct a summary of our dataset.

```{r echo = FALSE}
summary(df)
```

Upon looking at the summary of the given data, we notice that there are significant differences between the mean and median values (with higher mean than median values) for the koi_period, koi_depth, koi_srho, koi_prad, koi_insol, koi_sma and koi_dor variables, indicating right skewness in the distribution for these variables. There also appear to be strong outliers for many of the variables (such as koi_period, koi_prad and koi_depth), indicated by the large differences between the maximum and third quartile values for these variables. The data for variables such as koi_smass, koi_steff and koi_slogg hints at symmetric distributions, shown by the closeness between their mean and median values. 


#Univariate Analysis

Upon conducting our initial visualization, it was noticed that many of our predictor variables were significantly skewed. To ensure that our analysis is smoother, we may transform our right-skewed variables using log transformations. 

```{r, echo = FALSE}
predictors$koi_period = log(predictors$koi_period)
predictors$koi_depth = log(predictors$koi_depth)
predictors$koi_srho = log(predictors$koi_srho)
predictors$koi_prad = log(predictors$koi_prad)
predictors$koi_insol = log(predictors$koi_insol)
predictors$koi_dor = log(predictors$koi_dor)
predictors$koi_sma = log(predictors$koi_sma)
predictors$koi_duration = log(predictors$koi_duration)
df = data.frame(predictors,"y"=response)
```

##Orbit-related variables
We may begin by creating diagrams to observe the distribution of our variables. To start with, we will look at orbit-related variables (koi_dor, koi_period, koi_sma and koi_incl). Following our transformations, koi_dor, koi_period, and koi_sma appeared to have roughly symmetric distributions, so we will instead look closer at koi_incl.
```{r echo = FALSE, message = FALSE, warning = FALSE}
ggplot(data = df, mapping = aes(x=koi_incl))+ geom_histogram(col="black",fill = "blue") + ggtitle("Distribution of koi_incl")
```

As shown above, koi_incl appears to have a significantly left-skewed and unimodal distribution.

##Transit/eclipse-related variables

Next, we may look at the distributions for the transit/eclipse-related variables, namely, koi_impact, koi_duration, and koi_depth. Focusing on koi_depth, we observe a roughly symmetric (and perhaps bimodal) distribution.

```{r echo = FALSE, message = FALSE, warning = FALSE}
ggplot(data = df, mapping = aes(x= koi_depth)) + geom_histogram(col = "black", fill="pink") + ggtitle("Distribution of koi_depth")

```

##Exoplanet property-related variables

Next, we may take a look at the exoplanet property-related variables, which are koi_ror, koi_prad, koi_teq and koi_insol. Focusing on koi_insol and koi_prad, we observe the following:

```{r echo = FALSE, message = FALSE, warning = FALSE}
pred.prop = predictors[,c(7,11)]
pred.prop = pred.prop %>% gather(.)
ggplot(data = pred.prop, mapping = aes(x=value))+ geom_histogram(col="black",fill = "plum3") + facet_wrap(~key,scales='free_x') + ggtitle("Distribution of koi_insol and koi_prad") 

```

The distribution of koi_insol still appears to be symmetric and unimodal, while that for koi_prad appears to be somewhat symmetric and bimodal.

##Host star property-related variables

Finally, we may look at the distributions for the host star property-related variables (koi_srho, koi_steff, koi_slogg, koi_smet, koi_srad and koi_smass). Focusing on koi_srho, we observe the following:

```{r echo = FALSE, message = FALSE, warning = FALSE}
ggplot(data = df, mapping = aes(x=koi_srho)) + geom_histogram(col = "black", fill = "lightcoral") + ggtitle("Distribution of koi_srho")

```

Our histogram indicates that koi_srho has a fairly symmetric and unimodal distribution, with the mode situated at roughly 0.

#Bivariate Analysis

Before going on to conduct principal components analysis, it is useful to identify if any of the variables are strongly correlated with one another so as to reduce our number of predictors. We can examine this by constructing a correlation plot for our data as shown below.

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(corrplot)
corrplot(cor(predictors), method = "ellipse")
```

As shown by the correlation plot above, there appear to be especially strong positive correlations between koi_impact and koi_ror, koi_sma and koi_period, and fairly strong positive correlations between koi_dor and koi_period, koi_sma and koi_dor, and koi_insol and koi_teq. There also appear to be fairly strong negative correlations between koi_period and koi_insol, koi_insol and koi_sma, as well as between koi_insol and koi_dor. 

We will observe the relationships between koi_impact and koi_ror, and between koi_sma and koi_period (as the positive correlation values between these variables were especially high), as well as the negative relationship between koi_period and koi_insol.

##Koi_ror vs. Koi_impact

```{r echo = FALSE}
ggplot(data = df, mapping = aes(x=koi_impact,y=koi_ror)) + geom_point(col="red") + ggtitle("koi_ror vs. koi_impact")
```

As shown by the scatterplot above, there appears to be a very strong positive relationship between koi_impact and koi_ror, with most of the data points occuring between 0 and around 12 (on both the y and x-axes). The relationship here appears to be rather deterministic, so it would be good for our analysis to remove one of the variables.

##Koi_period vs. Koi_sma

```{r echo = FALSE}
ggplot(data = df, mapping = aes(x=koi_sma,y=koi_period)) + geom_point(col="red") + ggtitle("koi_period vs. koi_sma")
```

The scatterplot shown above suggests a strong positive linear relationship between the two variables, with values becoming slightly less clustered and koi_sma and koi_period increase. This relationship also appears to be somewhat deterministic, so we may remove one of the variables from our dataset.

##Koi_insol vs. Koi_period

```{r echo = FALSE}
ggplot(data = df, mapping = aes(x=koi_period,y=koi_insol)) + geom_point(col="red") + geom_density2d() + ggtitle("koi_insol vs. koi_period")
```

There appears to be a fairly strong negative relationship between koi_period and koi_insol.

Because of the very high correlation values between koi_impact and koi_ror and the rather deterministic relationship between the two, we will remove koi_ror. We will also remove koi_sma due to the deterministic relationship between koi_sma and koi_period.

```{r echo = FALSE}
df = df[,-c(5,8)]
predictors = predictors[,-c(5,8)]
pred.train = pred.train[,-c(5,8)]
pred.test = pred.test[,-c(5,8)]
```

#PC Analysis

Given that we have a large number of variables in our dataset, conducting PCA would give us an idea of whether we could reduce the dimensionality of our predictor space. The dominant predictors for each principal component have been displayed below.

```{r echo = FALSE}
pr.out = prcomp(predictors, scale = TRUE)
pc.vec = c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15")
pred.vec = c("koi_insol", "koi_slogg","koi_srho","koi_depth","koi_steff","koi_impact","koi_smet","koi_incl","koi_srad","koi_srho","koi_smass", "koi_teq","koi_prad","koi_insol","koi_dor")
kable(cbind("Principle Component" = pc.vec, "Dominant Variable" = pred.vec), format = "html", caption = "Dominant Predictors for Principal Components") %>% kable_styling()
```

We may now construct a plot to observe the cumulative proportions of variance explained by our 15 principal components, so as to determine which ones we must retain:

```{r echo = FALSE}
pve = 100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
df.new = data.frame("x" = 1:15, "y" = cumsum(pve))
ggplot(data = df.new, mapping = aes(x,y)) + geom_point() + geom_line() + xlab("Cumulative PVE") + ylab("Principal Component")
```

As shown above, approximately 90% of the variance is explained by the first 7 principal components, so we do not have much need to retain the other 7. This is validated by our summary of pr.out, in which around 91.68% of the variance is explained by the first 7 principle components as shown below.

```{r echo = FALSE}
summary(pr.out)
```

We may create a new dataset with only our 7 retained principal components (which we will need when conducting logistic regression analysis).

```{r echo = FALSE}
set.seed(101)
pca.pred.df = data.frame(predict(pr.out, predictors)[,1:7])
pca.pred.train = pca.pred.df[indices,]
pca.pred.test = pca.pred.df[-indices,]
pca.df = data.frame(pca.pred.df, response)


```

##Classification Analyses

We may now attempt to apply classifications models to our data, and observe which one has the strongest performance (by identifying which ones yield the lowest misclassification rate). We will also note the area under the curve values for the ROC curves of each of our models.

We will attempt 6 different analyses: logistic regression, linear discriminant analysis, classification tree, random forest, Naive bayes, and XGBoost.

###Logistic Regression

We can start with a logistic regression model:

```{r warning = FALSE, message = FALSE}
set.seed(101)
#Using our main training and test data
glm.out = glm(resp.train~., data = pred.train, family = binomial)
summary(glm.out)
resp.prob = predict(glm.out,newdata=pred.test,type="response")
roc.glm = roc(resp.test, resp.prob)
J = roc.glm$sensitivities + roc.glm$specificities - 1
w = which.max(J)
thresh = roc.glm$thresholds[w]
glm.sens = roc.glm$sensitivities[w]
glm.spec = roc.glm$specificities[w]
resp.pred = rep(NA,length(resp.prob))
for ( ii in 1:length(resp.prob) ) {
  if (resp.prob[ii] < thresh) {
    resp.pred[ii] = "CONFIRMED"   
  } else {
    resp.pred[ii] = "FALSE POSITIVE"   
  }
}
tb = table(resp.pred, resp.test)
tb
mcr.glm = 1-sum(tb[1,1], tb[2,2])/sum(tb[1,1], tb[1,2], tb[2,1], tb[2,2])
```

```{r echo = FALSE, message=FALSE, warning = FALSE}
kable(col.names = c("MCR (in percent)","AUC"), cbind(round(mcr.glm,3)*100,round(roc.glm$auc,4)), format = "html") %>% kable_styling(full_width = FALSE, position = "left")
```

```{r warning = FALSE, message = FALSE}
#Using our PCA training and test data
set.seed(101)
glm.out.pca = glm(resp.train~., data = pca.pred.train, family = binomial)
summary(glm.out.pca)
resp.prob.pca = predict(glm.out.pca,newdata=pca.pred.test,type="response")
pca.roc.glm = roc(resp.test, resp.prob.pca)
J = roc.glm$sensitivities + roc.glm$specificities - 1
w = which.max(J)
thresh = roc.glm$thresholds[w]
pca.glm.sens = pca.roc.glm$sensitivities[w]
pca.glm.spec = pca.roc.glm$specificities[w]
resp.pred.pca = rep(NA,length(resp.prob.pca))
for ( ii in 1:length(resp.prob.pca) ) {
  if (resp.prob.pca[ii] < thresh) {
    resp.pred.pca[ii] = "CONFIRMED"   
  } else {
    resp.pred.pca[ii] = "FALSE POSITIVE"   
  }
}
tb.pca = table(resp.pred.pca, resp.test)
tb.pca
mcr.glm.pca = 1-sum(tb.pca[1,1], tb.pca[2,2])/sum(tb.pca[1,1], tb.pca[1,2], tb.pca[2,1], tb.pca[2,2])
```

```{r echo = FALSE, message=FALSE, warning = FALSE}
kable(col.names = c("MCR (in percent)","AUC"), cbind(round(mcr.glm.pca,3)*100,round(pca.roc.glm$auc,4)), format = "html") %>% kable_styling(full_width = FALSE, position = "left")
```

Since our MCR for logistic regression conducted on our regular training and test data (11.8%) is significantly lower than the MCR we receive when we conduct logistic regression on the training and test data we obtained through PCA (23.2%), we can continue to use our regular data when trying out potential models. Furthermore, we also observe that the AUC for logistic regression conducted on our regular training and test data is 0.9413, but only 0.8457 for logistic regression conducted on our PCA data.

###Linear Discriminant Analysis

Next, we can attempt to conduct a linear discriminant analysis:

```{r echo = FALSE, warning = FALSE, message = FALSE}
set.seed(101)
library(MASS)
lda.fit=lda(resp.train~ ., data = pred.train)
lda.pred=predict(lda.fit, pred.test, probability = TRUE)
lda.prob=lda.pred$posterior
roc.lda = roc(resp.test, lda.prob[,1])
J = roc.lda$sensitivities + roc.lda$specificities - 1
w = which.max(J)
thresh = roc.lda$thresholds[w]
lda.sens = roc.lda$sensitivities[w]
lda.spec = roc.lda$specificities[w]
resp.pred.lda = rep(NA,length(resp.test))
for ( ii in 1:nrow(lda.prob) ) {
  if (lda.prob[ii,1] < thresh) {
    resp.pred.lda[ii] = "FALSE POSITIVE"   
  } else {
    resp.pred.lda[ii] = "CONFIRMED"   
  }
}
tb2 = table(resp.pred.lda,resp.test)
tb2
mcr.lda = 1-sum(tb2[1,1], tb2[2,2])/sum(tb2[1,1], tb2[1,2], tb2[2,1], tb2[2,2])
kable(col.names = c("MCR (in percent)","AUC"), cbind(round(mcr.lda,3)*100,round(roc.lda$auc,4)), format = "html") %>% kable_styling(full_width = FALSE, position = "left")
```

Our LDA analysis gives us a misclassification rate of approximately 19.6%, which is higher than what we received for logistic regression. We also receive a lower AUC of only 0.9009, so we will not use this model.

###Classification Tree

Next, we may attempt to generate a classification tree.
```{r echo = FALSE, warning = FALSE, message = FALSE}
library(rpart)
library(rpart.plot)
set.seed(101)
tree.pred=rpart(resp.train~., data = pred.train)
resp.prob.tree = predict(tree.pred, newdata = pred.test)
roc.tree = roc(resp.test, resp.prob.tree[,1])
J = roc.tree$sensitivities + roc.tree$specificities - 1
w = which.max(J)
thresh = roc.tree$thresholds[w]
tree.sens = roc.tree$sensitivities[w]
tree.spec = roc.tree$specificities[w]
resp.pred.tree = rep(NA,length(resp.test))
for ( ii in 1:nrow(resp.prob.tree) ){
  if ((resp.prob.tree[ii,1])<thresh) {
    resp.pred.tree[ii] = "FALSE POSITIVE"   
  }else{
    resp.pred.tree[ii] = "CONFIRMED"   
  }
}
tb3 = table(resp.pred.tree, resp.test)
tb3
mcr.class = 1-sum(diag(tb3))/sum(tb3)
kable(col.names = c("MCR (in percent)","AUC"), cbind(round(mcr.class,3)*100,round(roc.tree$auc,4)), format = "html") %>% kable_styling(full_width = FALSE, position = "left")
rpart.plot(tree.pred)

```

According to our classification tree, the dominant predictors appear to be koi_prad, koi_dor, koi_period, koi_insol, koi_srho, koi_smet, and koi_depth.

Our misclassification rate with this model is approximately 11.8%, which is very similar to that yielded by our logistic regression model. Our ROC curve yields an AUC value of 0.9117.

###Random Forest

Now, we will use a random forest model.

```{r echo = FALSE, warning = FALSE, message = FALSE}
set.seed(101)
library(randomForest)
rf.out = randomForest(resp.train~.,pred.train, importance = TRUE, trees = 100)
resp.prob.rf = predict(rf.out, pred.test, type = "prob")
roc.rf = roc(resp.test, resp.prob.rf[,1])
J = roc.rf$sensitivities + roc.rf$specificities - 1
w = which.max(J)
thresh = roc.rf$thresholds[w]
rf.sens = roc.rf$sensitivities[w]
rf.spec = roc.rf$specificities[w]
resp.pred.rf = rep(NA,length(resp.test))
for ( ii in 1:nrow(resp.prob.rf) ){
  if ((resp.prob.rf[ii,1])<thresh) {
    resp.pred.rf[ii] = "FALSE POSITIVE"   
  }else{
    resp.pred.rf[ii] = "CONFIRMED"   
  }
}
tb.rf = table(resp.pred.rf, resp.test)
tb.rf
mcr.rf = 1-sum(diag(tb.rf))/sum(tb.rf)
kable(col.names = c("MCR (in percent)","AUC"), cbind(round(mcr.rf,3)*100,round(roc.rf$auc,4)), format = "html") %>% kable_styling(full_width = FALSE, position = "left")
varImpPlot(rf.out, main = "Variable Importance Plot for Random Forest Model")
```

Upon a variable importance plot, we see that koi_prad and koi_dor seem to hold the highest amount of importance amongst the predictor variables.

We find that our misclassification rate is significantly lower than for all the other models, at 6.9%. In addition to this, we receive a fairly high AUC value of 0.9758.

###Naive Bayes

We may now attempt to use a Naive Bayes classifier on our data.

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(naivebayes)
naive = naive_bayes(pred.train, as.factor(resp.train))
resp.prob.naive = predict(naive, pred.test, type = "prob")
roc.naive = roc(resp.test, resp.prob.naive[,1])
J = roc.naive$sensitivities + roc.naive$specificities - 1
w = which.max(J)
thresh = roc.naive$thresholds[w]
naive.sens = roc.naive$sensitivities[w]
naive.spec = roc.naive$specificities[w]
resp.pred.naive = rep(NA,length(resp.test))
for ( ii in 1:nrow(resp.prob.naive) ){
  if ((resp.prob.naive[ii,1])<thresh) {
    resp.pred.naive[ii] = "FALSE POSITIVE"   
  }else{
    resp.pred.naive[ii] = "CONFIRMED"   
  }
}
tb.naive = table(resp.pred.naive, resp.test)
tb.naive
mcr.naive = 1-sum(diag(tb.naive))/sum(tb.naive)
kable(col.names = c("MCR (in percent)","AUC"), cbind(round(mcr.naive,3)*100,round(roc.naive$auc,4)), format = "html") %>% kable_styling(full_width = FALSE, position = "left")
```
Our resulting MCR is 15.6%, while our AUC is 0.9175.

###XGBoost

Lastly, we may now try to use the XGBoost algorithm.

```{r echo = FALSE, warning = FALSE, message = FALSE}
set.seed(101)
library(xgboost)
train.mat = xgb.DMatrix(data = as.matrix(pred.train), label = as.numeric(resp.train) - 1)
test.mat =  xgb.DMatrix(data = as.matrix(pred.test), label = as.numeric(resp.test) - 1)
cv.new = xgb.cv(train.mat, nfold = 5, nround = 20, params = list(objective = "binary:logistic"), verbose = 0)
boost.new = xgboost(train.mat, nrounds = which.min(cv.new$evaluation_log$test_error_mean), verbose = 0)
resp.prob.xg = predict(boost.new, newdata = test.mat, type = "response")
roc.xg = roc(resp.test, resp.prob.xg)
J = roc.xg$sensitivities + roc.xg$specificities - 1
w = which.max(J)
thresh = roc.xg$thresholds[w]
xg.sens = roc.xg$sensitivities[w]
xg.spec = roc.xg$specificities[w]
resp.pred.xg = rep(NA,length(resp.test))
for ( ii in 1:length(resp.prob.xg) ) {
  if (resp.prob.xg[ii] < thresh) {
    resp.pred.xg[ii] = "CONFIRMED"   
  } else {
    resp.pred.xg[ii] = "FALSE POSITIVE"   
  }
}
tb4 = table(resp.pred.xg, resp.test)
tb4
mcr.boost = 1-sum(diag(tb4))/sum(tb4)
kable(col.names = c("MCR (in percent)","AUC"), cbind(round(mcr.boost,3)*100,round(roc.xg$auc,4)), format = "html") %>% kable_styling(full_width = FALSE, position = "left")


```

Our XGBoost algorithm yields a low MCR of 8.3% and a fairly high AUC value of 0.9673. However, these results are not as good as what we received using our random forest algorithm, so we will use our random forest model to make our final predictions.

Upon attempting ridge regression and lasso analyses, misclassification rates of 15% and 12% respectively were observed. Since these were both higher than for some of our other models, these models were discarded. Best subset selection was also considered, but due to the considerably large sample size of our data, it was too computationally inefficient to execute.

##Conclusion

After attempting to fit six potential models to our data, namely logistic regression, a classification tree, linear discriminant analysis, Naive Bayes, XGBoost, and random forest analysis, we found that using a random forest model on our data yielded the lowest misclassification rate, the highest AUC and the highest sensitivity (as shown below). The resulting ROC curve for this model has also been displayed. This suggests to us that a random forest model is likely the best possible option that would allow us to effectively categorize our exoplanetary candidates as either "CONFIRMED" or "FALSE POSITIVE".

```{r echo = FALSE}
mcr.vec = c(mcr.glm, mcr.glm.pca, mcr.class, mcr.lda, mcr.rf, mcr.naive, mcr.boost)
names.vec = c("Logistic Regression", "Logistic Regression with PCA Data", "Classification Tree", "LDA Analysis", "Random Forest Analysis", "Naive Bayes", "XGBoost")
auc.vec = c(roc.glm$auc, pca.roc.glm$auc, roc.tree$auc, roc.lda$auc, roc.rf$auc, roc.naive$auc, roc.xg$auc)
spec.vec = c(glm.spec, pca.glm.spec, tree.spec, lda.spec, rf.spec, naive.spec, xg.spec)
sens.vec = c(glm.sens, pca.glm.sens, tree.sens, lda.sens, rf.sens, naive.sens, xg.sens)
mcr.data = cbind("Name of Model" = names.vec, "MCR (in percent)" = round(mcr.vec,3)*100, "AUC (in percent)" = round(auc.vec,3)*100, "Sensitivity (in percent)" = round(sens.vec,3)*100, "Specificity (in percent)" = round(spec.vec,3)*100)
kable(mcr.data, format = "html", caption = "Performance across Classifiers") %>% kable_styling()
plot(roc.rf, col = "red", main = "ROC Curve for Random Forest Model")
```

#Making Predictions

We will now use our random forest model to generate predictions for our CANDIDATE data.

```{r echo = FALSE}
df.candidate = df.candidate[,-c(5,8)]
pred.candidate = predict(rf.out, df.candidate)
table(pred.candidate)
```

As indicated above, it appears that our model has classified 1112 candidates as having confirmed exoplanets, and 1206 candidates as 'false positive' observations (i.e. not having exoplanets).